---
title: "Federated LLM Fine-tuning with Flower"
collection: publications
category: conferences
permalink: /publication/2024-KICS
excerpt: 'Large language models (LLMs) trained on public data show high performance, but the scarcity of quality data and the need for domain-specific models are becoming prominent. Federated learning-based LLM fine-tuning provides a solution by enabling collaborative model improvement without sharing raw data, protecting privacy while creating domain-specific models. In this study, we used the Flower framework to fine-tune a medical domain-specific model and analyzed the impact of model parameters, client numbers, training rounds, and quantization levels on the performance and efficiency of federated learning.'
date: 2024-11-21
venue: 'Korean Institute of Communications and Information Sciences (KICS)'
logo: '<img src="/images/kics.jpeg" style="width:200px;">'
paperurl: 'https://drive.google.com/file/d/1h7783wPBh6x91WVwyPl0Oniji0ncDqjj/view?usp=drive_link'
citation: 'J. Jung (2024). &quot;Federated LLM Fine-tuning with Flower; <i>Korean Institute of Communications and Information Sciences (KICS)</i>.'

---

Large language models (LLMs) have demonstrated remarkable performance across various machine learning tasks, playing a key role in natural language processing. However, the scarcity of high-quality public data [1] and the growing demand for domain- specific models, especially in fields like healthcare, law, and finance, highlight the need for specialized approaches. Federated learning offers a solution by enabling collaborative model improvement without sharing raw data, thus protecting privacy while supporting the creation of domain-specific models.
Federated fine-tuning of LLMs allows organizations to improve models for specialized domains like healthcare without data sharing. The Flower framework [2] facilitates this by enabling scalable federated learning across multiple clients. In this study, we employ the Flower framework to fine-tune an LLM for the medical domain and experimentally analyze how model parameters, client numbers, training rounds, and quantization levels affect federated learning performance. This research aims to uncover how these factors contribute to the efficiency and performance of federated learning systems.

<br>
